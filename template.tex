

\title{Video salient objects detection via inter-frame motion feature enhancement}
\author{
        Vitaly Surazhsky \\
                Department of Computer Science\\
        Technion---Israel Institute of Technology\\
        Technion City, Haifa 32000, \underline{Israel}
            \and
        Yossi Gil\\
        Department of Computer Science\\
        Technion---Israel Institute of Technology\\
        Technion City, Haifa 32000, \underline{Israel}
}
\date{\today}

\documentclass[12pt]{article}
\usepackage{comment}
\usepackage{amsmath,bm}
\begin{document}
\maketitle

\begin{abstract}
Since the employment of deep learning model in video salient detection, the accuracy and efficiency of the recent approaches have reached a very high level. However, as for the exploration of motion features and spatiotemporal fusion, these still exists scope for improvement. In this paper, we propose a densely motion feature enhancement network for video salient object detection. Firstly, a densely motion feature enhancement module is employed to solve improve the robustness of inter-frame motion features. In this module, we refine the motion features by dense integration of infer-frame deep features and historic salieny supervision. After that, to explore the spatiotemporal contextual relationship, we propose a sptiotemporal cross-attention block, which selectively integrates the both spatial and temporal features by computing the weights of these features. The experiments demonstrate that our approach has the competitive performances on 7 widely used datasets, i.e. ViSal, SegTrack and VOS. Meanwhile, the proposed network is an end-to-end framework without extra supplementary in both training and testing phases. As for the community of unsupervised video object segmentation, we also achieve the competitive results DAVIS dataset against the state-of-the-arts. 
\end{abstract}

\section{Introduction}
The purpose of salient object detection (SOD) is to focus on the most attractive objects or regions in an image or video and then highlight them from the complex background. As the trait of this community, it usually is treated as the pre-processing to support other visual tasks, such as visual tracking, image retrieval and action recognition. According to the input of frameworks, SOD can be categoried into image salient object detection and video salient object detection. In this paper, our research emphasizes on the more complex video salient object detection.

The development of video saliency approaches can be divided into two stages. The first stage is based on hand-crafted features and some heuristics \cite{xxx}. As the shoratage of robust motion features and efficient spatiotemporal models, these methods are not able to handle complex video scenes, such as motion blur, low contrast and so on. With the employment of deep learning models, especially fully convolutional networks (FCNs)\cite{xxx}, the accuracy and efficiency of this field have achieved a top level. Moreover, the recent approaches, PDB \cite{xxx} and SSAV \cite{xxx}, outperform previous methods with a largin margin, by using the dilated convolutions and LSTM-based module to extract the robust spatiotemporal features. Under this kind of senario, to improve the accuracy on these approaches, we need to further refine the motion features and explore the relationship between spatial and temporal information.

Through the analysis of the intermediate results in deep FCN model, we notice that there exists the fluctuation of feature values between two consecutive frames in few video sequences. As shown in Fig.\ref{results}, the first row is the consecutive input frames. The second row and the third row are the energy visualization of the corresponding intermediate feature maps and final saliency maps (with sigmoid activation), respectively. Compared with colar bar beside the energy visualization, the feature values of the second frame is obviously smaller than the first ones. Moreover, some feature values are less than one, which leads to the loss of the salient object in the saliency map.  

Notice the previous the state-of-the-arts (i.e., PDB and SSAV), their frameworks consists of spatial learning block and temporal learning block. The former one is to extract the static features in an single frame, the latter one is to refine the dynamic features after the spatial learning block. This kind of framework is efficient and can extract robust spatiotemporal features, but it still does not fully explore the contextual dependencies between spatial and temporal features, which is able to produce more robust fusing spatotemporal features and further improve the quality of saliency maps.

In this paper, we propose an end-to-end deep framework, called densely motion feature enhancement network (MEN). Observed by the fluctuation of feature values in Fig.\ref{results}, we try to exploit the features of previous frame to aggregate the current ones. Therefore, an inter-frame motion features enhancement module is introduced to refine the motion information. In this module, the recursively aggregated feature maps and historic saliency supervision are employed to provide sufficient motion information for current frame. Besides, to explore the contextual dependencies between the spatial and temporal features, we introduce a sptiotemporal cross-attention block. Inspired by position attention in \cite{}, the proposed attention block generates the weight of feature by any positions with similar features from both spatial and temporal information. Togethter with motion features enhancement module, our MEN is able to effetively revise the fluctuation situation and produce more robust spatiotemporal feature for video saliency estiamtion.

In summary, this paper can conclude the following contributions:

\begin{itemize}
  \item We proposed a end-to-end densely motion feature enhancement network, without any extra supplementary information in training and testing stage. Additionally, this approach achieve competitive performance in the field of video salient object detection (VSOD) and unsupervised video object segmentation (UVOS). 
  \item We introduce an inter-frame motion features enhancement module, which refines the motion features by the recursively aggregated feature maps and historic saliency supervision.
  \item To fully exploit the spatitemporal information and explore their contextual dependencies, a spatiotemporal cross-attention block is proposed to combine the spatial and temporal information together.	  
\end{itemize}
 

\section{Related Work}
\subsection{Video salient object detection}

With the introduction of deep learning methods, we can divide prvious video saliency approaches into two phases, which are non-deep learning approaches and deep learning-based approaches. The formers are based on hand-crafted features (e.g., color contrast, texture, optical flow) and heuristic models (e.g., gesidesc distance, center-surround contrast). As the exploitation of optical flow and optimization models, these approaches are usually time-consuming. Moreover, they also cannot conduct the complex video scenes such as motion blur, low contrast, occlusion, because of the limitation of hand-crafted features. With the success of deep learning in many visual tasks, this community also starts to widely use deep learning \cite{A,B,C,D,F}. At the beginning, as the shortage of training data, weakly supervised method \cite{scnn} and data synthetic \cite{fcn} are employed to supply pixel-wise labels. In the aspect of deep saliency models, dilated convolutions and LSTM-based structures are introduced to retain sufficient feature scale and extract motion information, respectively. For example, PDB \cite{pdb} extracts spatial features by pyramid dilated convolutions and obtains temporal features by deeper bi-directional ConvLSTM structure. Based on PDB, SSAV \cite{ssav} further introduces eye fixation records for network training and proposes a saliency-shift-aware convLSTM module to capture video saliency dynamics. Additionally, DLVS \cite{scnn} proposes a saliency detection network by stepwise extracting the static and dynamic information. Li et al. \cite{fgrne} subtly combines optical flow and short connection structure \cite{dss} to propose a flow guided recurrent neural encoder framework. SCOM \cite{scom} and STCR \cite{scom} build different optimization models, but the employment of deep features make them achieve good performances. 

\subsection{Unsupervised video object segmentation}

Video saliency detection is very similar with Unsupervised video segmentation. The former one is to generate the probability value of each pixel in video frames, the latter one is to classify each pixel. Like the development of video saliency, early video segmentations mainly integrates hand-carfted features and heuristics, which include clustering \cite{keuper2015motion,brox2010object,chang2013video}, objectness \cite{lee2011key,ma2012maximum,koh2017primary,li2018instance} and saliency guidance \cite{hu2018unsupervised,wang2015consistent,wang2015robust}. As the limiting robustness of hand-crafted features and the low speed of optical flow extraction, these approaches have gradually reached the bottleneck. To obtain more robust spatiotemporal features, recent works begin to build their frameworks with convolutional neural networks (CNNs). FusionSeg \cite{jain2017fusionseg} proposes a two-stream framework, which extracts spatial and temporal cues, respectively. In \cite{perazzi2017learning,Tokmakov_2017_CVPR,Cheng_2017_ICCV}, optical flow is widely used to extract motion information for video segmentation. Further, FGRNE \cite{fgrne} incorporate LSTM and optical flow to generate robust motion features. Latest, AGS \cite{AGS} collects UVOS-aware human attention data and then train the attention-guided object segmentation network with this kind of data. In this paper, we also treat video segmentation as an extra validation task, which proves our approache is feasible in this community.  

\subsection{Visual attention}

Attention mechanism is to compute the weights from different and complex information. It usually is used for selecting and fusing features in many visual tasks, such as natural language processing \cite{lin2017structured, vaswani2017attention}, image caption \cite{chen2017sca,lu2016hierarchical}, image segmentaion \cite{fu2019dual,yuan2018ocnet}, etc. In particular, the work \cite{yuan2018ocnet} proposes an object context pooling scheme by exploiting self-attention module, whose principle is to compute similarites of all pixels and them selectively integrates them. DANet \cite{fu2019dual} further propose two kinds of attention modules: position attention module (PAM) and channel attention module (CAM). The two modules can explore the contextual dependencies in FCN framework. Inspired by their PAM, we build a cross-attention module to extract the contextual information in spatial and temporal cues. 

\section{Densely motion feature enhancement network}

\subsection{Overview}

The proposed framwork mainly consists of four components. As shown in Fig.\ref{framework}, it includes backbone network, initial motion extractor, inter-frame motion features enhancement module and spatiotemporal cross-attention block. Given the video frames, the spatial features are firstly extracted by the backbone network. Here, to fully exploit the multi-scale features, we follow the feature encoding method \cite{fu2019dual} to merge the feature maps from the last two convlutional blocks of ResNeXt. The merged feature maps are fed into the atrous spatial pyramid pooling module (ASPP) \cite{chen2017rethinking} to refine spatial features. After that, we choose a convGRU layer as the initial motion feature extractor, because its parameters are less and it is able to use a small amount of data to complete training. Next, the initial motion features can be recursively aggregated by the proposed motion feature enhancement module. Additionally, to incorporate the spatial and temporal features, the cross-attention block is embedding after the initial motion extractor. At the top of the framework, the genrated saliency maps are used to compute the loss with the ground truths by sigmoid cross-entropy function, thus optimizing the whole neural network.  

\subsection{Motion feature enhancement}

To solve the fluctuation of feature values in consecutive frames, we propose the inter-frame motion feature enhancement module, which straightforward yet effective. The main idea of this module is to recursively aggregate feature maps and to exploit the historic saliency guidance. Specifically, given the consecutive feature maps $\{..., M_{t-1}, M_{t}, M_{t+1}, ...\}$ from the initial motion extractor, the current feature maps $M_{t}$ are directly added by the previous ones $M_{t-1}$, thus generating the enhanced features $E_{t}$. Meanwhile, inspire by \cite{deng2018r3net}, we introduce a saliency guidance block, which treats the historic saliency maps as an extra supervision information $A_t$ to guide the network learning. The whole process of the motion feature enhancement can be formulated as follows:  

\begin{equation}
\label{enhance}
\begin{aligned}
   P_{t}  &= \Gamma(Cat(E_{t}, A_{t-1});W_m) \\
   E_{t}  &= M_{t-1} + M_{t} \\
   A_{t-1}  &= G(P_0, ..., P_{t-2};W_s); t \in \bm{N_{+}}
\end{aligned}
\end{equation} where $P_{t}$ and $E_{t}$ and final saliency prediction and enhanced feature maps of current frame. $A_{t-1}$ represents the historic saliency supervision of previous frames. $\Gamma(\cdot; \cdot)$ denotes a group of convolutional operations, which include three standard convolution layers with batch normalization and PReLU activation. Their corresponding convolutional kernals are $3 \times 3$, $3 \times 3$ and $1 \times 1$, respectively. $G(\cdot; \cdot)$ is the squeeze-and-excitation operation \cite{hu2018squeeze}, which is detailedly described later. $W_m$ and $W_s$ are the learnable parameters. 

Historic saliency supervision. Inspired by \cite{deng2018r3net} and \cite{wang2016saliency}, the saliency results can be regarded as a supervision signal to guide the nework learning. In this module, to fully exploit the sequantial information of the consecutive saliency maps, we introduce the squeeze-and-excitation operation to integrate them. The operation $G(\cdot; \cdot)$ can be described as below. Firstly, an average pooling is used to squeeze the feature maps $\{P_1, P_2 ..., P_{i-1}\}$ in the saliency guided block: 

\begin{equation}
\label{sq}
\begin{aligned}
   z_{c} =f_{sq}(P_c) = \frac{1}{H \times W} \sum\limits_{i=1}^{H} \sum\limits_{j=1}^{W} P_c; c &\in \{0,1,...,t-1\}
 \end{aligned}
\end{equation} where the historic saliency maps $P_c \in R^{C \times W \times H}$ are converted into the $z_c \in R^{C}$. Then, two fully connected layers (FC) are employed to learn the nonlinear interaction between channels. This process can be written as below:

\begin{equation}
\label{ex}
\begin{aligned}
   s = f_{ex}(\bm{z}, \textbf{W}) = \sigma(g(\bm{z}, \bm{W})) = \sigma(\bm{W_2}\delta(\bm{W_1}\bm{z}))
 \end{aligned}
\end{equation} $s$ is the learned weight vector. The whole excitation can be unfolded into two FCs. $\bm{W_1} \in R^{\frac{C}{r}\times C} $ and $\bm{W_2} \in R^{C\times \frac{C}{r}} $ represent the their parameters, where $r$ is the reduction rate. $\sigma(\cdot)$ and $\delta(\cdot)$ demonstrate ReLU and sigmoid activation, respectively. After that, the weighted feature maps can be obtained by a channel-wise multiplication ($f_{s}(\cdot)$ in Eg.\ref{scale}). Finally, the weighted feature maps are weighed sum up to the supervision to guide the learning of next frame with the Eq.\ref{sum_scale}.

\begin{equation}
\label{scale}
\begin{aligned}
   \bm{\tilde{P}_c} = f_{s}(\bm{P}_c, s_c) = s_c \cdot \textbf{P}_c
\end{aligned}
\end{equation}

\begin{equation}
\label{sum_scale}
\begin{aligned}
   A_i = \sum\limits_{c=1}^{t-1}\tilde{P}_c
 \end{aligned}
\end{equation}
 
\subsection{Cross-attention block} 

This block is embedded between the initial motion feature extractor and motion feature enhancement module. Its purpose is to explore the contextual relationship between spatial and temporal features. The specific process is shown in Fig.\ref{sta}. The input of this block comes from the backbone network and initial motion feature extractor, which represent the spatial $I_s \in R^{C \times W \times H}$ and temporal features $I_t \in R^{C \times W \times H}$, respectively. The spatial features are used to generate a query feature maps $Q_s$ by a convolution layer (1 $\times$ 1 kernel). Then, the query feature maps are reshaped to $R^{C \times N}$, where $N = W \times H $. Meanwhile, the temporal feature maps are converted to a key feature maps $K_t \in R^{N \times C}$. After that, the transpose of $Q_s$ is multiplied by $K_t$, thus generating the attention map $O \in R^{N \times N}$ by a softmax activation.

\begin{equation}
\label{sta}
\begin{aligned}
   o_{ji} = \frac{exp(q_i \cdot k_j)}{\sum_{i=1}^{N}exp(q_i \cdot k_j)}
 \end{aligned}
\end{equation} where $q_i$ and $k_j$ are the values in $Q_s$ and $K_t$, respectively. $o_{ji} \in O$ denotes the learn weight of i-th position in spatial features and j-th position in temporal features. Finally, the value feature maps ($V_s and V_t$) of spatial and temporal features are generated by two convolution layers and then reshaped to $R^{C \times N}$. The fusing spatiotemporal features $F$ are produced by the formula as follows:

\begin{equation}
\label{sta}
\begin{aligned}
   F = Re(O \cdot V_s)+ I_s + Re(O \cdot V_t)+ I_t 
 \end{aligned}
\end{equation} where $Re(\cdot)$ is the reshape operation, which turns the result of matrix multiplication between learned weight and valure feature maps ($V_s$ and $V_t$ ) back to $R^{C \times W \times H}$. Then, an element-wise addition is employed to fuse all these feature maps.
 
\begin{comment}
@inproceedings{wang2016saliency,
  title={Saliency detection with recurrent fully convolutional networks},
  author={Wang, Linzhao and Wang, Lijun and Lu, Huchuan and Zhang, Pingping and Ruan, Xiang},
  booktitle={European conference on computer vision},
  pages={825--841},
  year={2016},
  organization={Springer}
}

@inproceedings{hu2018squeeze,
  title={Squeeze-and-excitation networks},
  author={Hu, Jie and Shen, Li and Sun, Gang},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={7132--7141},
  year={2018}
}

@article{chen2017rethinking,
  title={Rethinking atrous convolution for semantic image segmentation},
  author={Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  journal={arXiv preprint arXiv:1706.05587},
  year={2017}
}

@inproceedings{deng2018r3net,
  title={R3Net: Recurrent residual refinement network for saliency detection},
  author={Deng, Zijun and Hu, Xiaowei and Zhu, Lei and Xu, Xuemiao and Qin, Jing and Han, Guoqiang and Heng, Pheng-Ann},
  booktitle={Proceedings of the 27th International Joint Conference on Artificial Intelligence},
  pages={684--690},
  year={2018},
  organization={AAAI Press}
}
	
@inproceedings{fu2019dual,
  title={Dual attention network for scene segmentation},
  author={Fu, Jun and Liu, Jing and Tian, Haijie and Li, Yong and Bao, Yongjun and Fang, Zhiwei and Lu, Hanqing},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={3146--3154},
  year={2019}
}

@article{yuan2018ocnet,
  title={Ocnet: Object context network for scene parsing},
  author={Yuan, Yuhui and Wang, Jingdong},
  journal={arXiv preprint arXiv:1809.00916},
  year={2018}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}
@article{lin2017structured,
  title={A structured self-attentive sentence embedding},
  author={Lin, Zhouhan and Feng, Minwei and Santos, Cicero Nogueira dos and Yu, Mo and Xiang, Bing and Zhou, Bowen and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1703.03130},
  year={2017}
}

@inproceedings{fu2017look,
  title={Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition},
  author={Fu, Jianlong and Zheng, Heliang and Mei, Tao},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4438--4446},
  year={2017}
}
@inproceedings{lu2016hierarchical,
  title={Hierarchical question-image co-attention for visual question answering},
  author={Lu, Jiasen and Yang, Jianwei and Batra, Dhruv and Parikh, Devi},
  booktitle={Advances In Neural Information Processing Systems},
  pages={289--297},
  year={2016}
}
@inproceedings{chen2017sca,
  title={Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning},
  author={Chen, Long and Zhang, Hanwang and Xiao, Jun and Nie, Liqiang and Shao, Jian and Liu, Wei and Chua, Tat-Seng},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5659--5667},
  year={2017}
}

@inproceedings{keuper2015motion,
  title={Motion trajectory segmentation via minimum cost multicuts},
  author={Keuper, Margret and Andres, Bjoern and Brox, Thomas},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={3271--3279},
  year={2015}
}

@inproceedings{brox2010object,
  title={Object segmentation by long term analysis of point trajectories},
  author={Brox, Thomas and Malik, Jitendra},
  booktitle={European conference on computer vision},
  pages={282--295},
  year={2010},
  organization={Springer}
}

@inproceedings{chang2013video,
  title={A video representation using temporal superpixels},
  author={Chang, Jason and Wei, Donglai and Fisher, John W},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2051--2058},
  year={2013}
}

@inproceedings{lee2011key,
  title={Key-segments for video object segmentation},
  author={Lee, Yong Jae and Kim, Jaechul and Grauman, Kristen},
  booktitle={2011 International conference on computer vision},
  pages={1995--2002},
  year={2011},
  organization={IEEE}
}

@inproceedings{koh2017primary,
  title={Primary object segmentation in videos based on region augmentation and reduction},
  author={Koh, Yeong Jun and Kim, Chang-Su},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={7417--7425},
  year={2017},
  organization={IEEE}
}

@inproceedings{ma2012maximum,
  title={Maximum weight cliques with mutex constraints for video object segmentation},
  author={Ma, Tianyang and Latecki, Longin Jan},
  booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition},
  pages={670--677},
  year={2012},
  organization={IEEE}
}

@inproceedings{li2018instance,
  title={Instance embedding transfer to unsupervised video object segmentation},
  author={Li, Siyang and Seybold, Bryan and Vorobyov, Alexey and Fathi, Alireza and Huang, Qin and Jay Kuo, C-C},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6526--6535},
  year={2018}
}

@article{wang2015robust,
  title={Robust video object cosegmentation},
  author={Wang, Wenguan and Shen, Jianbing and Li, Xuelong and Porikli, Fatih},
  journal={IEEE Transactions on Image Processing},
  volume={24},
  number={10},
  pages={3137--3148},
  year={2015},
  publisher={IEEE}
}

@article{wang2015consistent,
  title={Consistent video saliency using local gradient flow optimization and global refinement},
  author={Wang, Wenguan and Shen, Jianbing and Shao, Ling},
  journal={IEEE Transactions on Image Processing},
  volume={24},
  number={11},
  pages={4185--4196},
  year={2015},
  publisher={IEEE}
}

@inproceedings{hu2018unsupervised,
  title={Unsupervised video object segmentation using motion saliency-guided spatio-temporal propagation},
  author={Hu, Yuan-Ting and Huang, Jia-Bin and Schwing, Alexander G},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={786--802},
  year={2018}
}

@inproceedings{jain2017fusionseg,
  title={Fusionseg: Learning to combine motion and appearance for fully automatic segmentation of generic objects in videos},
  author={Jain, Suyog Dutt and Xiong, Bo and Grauman, Kristen},
  booktitle={2017 IEEE conference on computer vision and pattern recognition (CVPR)},
  pages={2117--2126},
  year={2017},
  organization={IEEE}
}

@inproceedings{perazzi2017learning,
  title={Learning video object segmentation from static images},
  author={Perazzi, Federico and Khoreva, Anna and Benenson, Rodrigo and Schiele, Bernt and Sorkine-Hornung, Alexander},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2663--2672},
  year={2017}
}

@InProceedings{Tokmakov_2017_CVPR,
author = {Tokmakov, Pavel and Alahari, Karteek and Schmid, Cordelia},
title = {Learning Motion Patterns in Videos},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017}
}

@InProceedings{Cheng_2017_ICCV,
author = {Cheng, Jingchun and Tsai, Yi-Hsuan and Wang, Shengjin and Yang, Ming-Hsuan},
title = {SegFlow: Joint Learning for Video Object Segmentation and Optical Flow},
booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
month = {Oct},
year = {2017}
}

\end{comment}

\section{Results}\label{results}
In this section we describe the results.

\section{Conclusions}\label{conclusions}
We worked hard, and achieved very little.

\bibliographystyle{abbrv}
\bibliography{main}

\end{document}
